
seed: &seed 3407

n_class: &n_class 8
#n_class: &n_class 2

weight_decay: &weight_decay 1.0e-4
monitor_rate: {train: 60.0, test: 1.0}

phases: [train, test]
is_plot: False
disable_tb: True
disable_tqdm: False
metrics: [loss, t1_acc, t2_acc, g_norm, p_norm, lr]
aux_to_metric: [load_loss, imp_loss, expert_label_loss]
metrics_print: [loss, t1_acc, t2_acc, g_norm]


setup:
  #xargs: {x: obj_x, mask: obj_mask, one_hot: obj_one_hot, glob: glob}
  #xargs: {feature: feature, mask: mask, position: position, one_hot: one_hot}
  #xargs: {feature: feature, mask: mask, one_hot: one_hot}
  xargs: {feature: feature, mask: mask, position: position, one_hot: one_hot, glob: glob}
  #xargs: {feature: feature, mask: mask, position: position}
  layers:
    model: ViT-L:4-D:32
    norm: LN-CS:1-CO:1
    pooling: GAP-A:1
    head: MLP-L:1-D:32
    MoE: Token-NC:2-E:4-K:2-C:8
    FiLM: FiLM-L:1-D:8
  model_name: Nominal
  optimizer: adam
  lr_scheduler: warmup_cosine_decay_schedule
  loss: cb_bce_loss
  #loss: ce_loss
  n_class: *n_class
  flooding: null
  seed: *seed
  log_param: False
  dataset: 
    config: dataset.yaml
    name: hzqcd_pn_v1
    #name: top_data_v1
  n_data:
    info_step: 1024
    n_step: {train: 800000, test: 512} # 32 epoch
    split_N: 128
    split_idx: 0
    n_files: 2064
    repeat: True
  workdir: result
  mixed_precision: False
  smooth_label: False
  weight_decay: *weight_decay
  alpa:
    num_micro_batches: 8
    data_parallel: -1
    operator_parallel: 1
    pipeline_parallel: 1


n_data_book:
  n_pn_top: 
    info_step: 3154
    n_step: {train: 63080, test: 404} # 32 epoch
    split_N: 128
    split_idx: 0
    n_files: 2064
    repeat: True
  lr_range_test: 
    info_step: 100
    n_step: {train: 110000, test: 404} # 32 epoch
    split_N: 128
    split_idx: 0
    n_files: 2064
    repeat: True
  n2048: 
    info_step: 1024
    n_step: {train: 1048576, test: 512} # 32 epoch
    split_N: 128
    split_idx: 0
    n_files: 2064
    repeat: True
  n1024: 
    info_step: 1024
    n_step: {train: 204800, test: 512} # 12.5 epoch
    split_N: 64
    split_idx: 0
    n_files: 1040
    repeat: True
  n256:
    info_step: 1024
    n_step: {train: 102400, test: 512}
    split_N: 16
    split_idx: 0
    n_files: 272
    repeat: True
  n64:
    info_step: 1024
    n_step: {train: 102400, test: 128}
    split_N: 16
    split_idx: 0
    n_files: 68
    repeat: True

mp_policy: 
  nonfinite: True
  init_val: 32768 #2**15

optimizer:
  opts:
    clip_by_global_norm: {max_norm: 100.0}
  radam: {b1: 0.90, b2: 0.999}
  adam: {b1: 0.90, b2: 0.999}
  adamw: {b1: 0.90, b2: 0.999, weight_decay: 1.0e-4}

lr_scheduler:
  warmup_cosine_decay_schedule: 
    init_value: 1.0e-8
    peak_value: 1.0e-3
    warmup_steps: 0.1
    decay_steps: 1.0
    end_value: 1.0e-8
  warmup_cosine_decay_schedule:2: 
    init_value: 1.0e-7
    peak_value: 1.0e-5
    warmup_steps: 0.10
    decay_steps: 1.0
    end_value: 1.0e-7
  linear_onecycle_schedule:
    transition_steps: 1.0
    peak_value: 3.0e-3
    pct_start: 0.4
    pct_final: 0.6
    div_factor: 10.0
    final_div_factor: 1.0e+3
  linear_schedule:
    init_value: 1.0e-2
    end_value: 1.0e-7
    transition_steps: 1.0
    transition_begin: 0
  constant_schedule:
    value: 1.0e-5
  step_lr:
    values: [1.0e-7, 1.0e-6, 1.0e-5, 1.0e-4, 1.0e-3, 1.0e-2, 1.0e-1]
    steps:  [0, 10000, 15000, 20000, 25000, 30000, 35000]
  lr_range_test:
    init_lr: 1.0e-8
    last_lr: 1.0e-1
    init_step: 10000
    step_size: 100
    n_step: 1000
    


loss:
  cb_bce_loss: {beta: 0.9999, gamma: 2.0}
  cb_ce_loss: {beta: 0.9999, gamma: 0.5}
  ce_loss: None


hparams:
  weight_decay: *weight_decay
  setup: [loss, lr_scheduler, optimizer]


FiLM:
  MLP: 
    depth: [int, 4, L]
    dim: [int, 4, D]
    expansion: [int, 2, E]
    norm: [bool, 0, NM]
    activation: [bool, 1, AC]
    dropout: [float, 0.0, DR]
    w_init: [str, null, WI]

MoE:
  Token: 
    moe: [str, token, T]
    cycle: [int, 2, NC]
    n_experts: [int, 16, NE]
    topK: [int, 2, K]
    capacity: [int, 4, C]
    loss_w_imp: [float, 1.0e-2, LI]
    loss_w_load: [float,1.0e-2, LL]
    w_init: [str, null, WI]
  SMoE: 
    moe: [str, SMoE, T]
    cycle: [int, 2, NC]
    n_experts: [int, 16, NE]
    topK: [int, 2, K]
    capacity: [int, 4, C]
    loss_w_imp: [float, 1.0e-2, LI]
    loss_w_load: [float,1.0e-2, LL]
    w_init: [str, null, WI]
  XMoE: 
    moe: [str, XMoE, T]
    cycle: [int, 2, NC]
    n_experts: [int, 16, NE]
    topK: [int, 1, K]
    capacity: [int, 4, C]
    dim_e_factor: [int, 2, DE]
    loss_w_load: [float,1.0e-2, LL]
    w_init: [str, null, WI]
  ECMoE: 
    moe: [str, ECMoE, T]
    cycle: [int, 2, NC]
    n_experts: [int, 16, NE]
    topK: [int, 1, K]
    loss_w_imp: [float, 0.0, LI]
    loss_w_load: [float,0.0, LL]
    loss_w_label: [float,0.0, LS]
    w_init: [str, null, WI]

norm:
  IN:
    type: [str, InstanceNorm, T]
    create_scale: [bool, 1, CS]
    create_offset: [bool, 1, CO]
    data_format: [str, channel_last, DF]
  BN:
    type: [str, BatchNorm, T]
    create_scale: [bool, 1, CS]
    create_offset: [bool, 1, CO]
    decay_rate: [float, 0.0, DR]
    data_format: [str, channel_last, DF]
  LN:
    type: [str, LayerNorm, T]
    axis: [int, -1, AX]
    create_scale: [bool, 1, CS]
    create_offset: [bool, 1, CO]
  GN:
    type: [str, GroupNorm, T]
    groups: [int, 4, G]
    axis: [int, -1, AX]
    create_scale: [bool, 1, CS]
    create_offset: [bool, 1, CO]
  GRN:
    type: [str, GRN, T]
    eps: [float, 1.0e-6, E]

head:
  MLP:
    depth: [int, 1, L]
    dim: [int, 16, D]
    expansion: [int, 4, E]
    dropout: [float, 0.1, DR]
    drop_path: [float, null, DP]
    layer_scale: [float, null, LS]
    focal: [bool, 0, FC]
    w_init: [str, null, WI]
  PN:
    depth: [int, 1, L]
    dim: [int, 256, D]
    dropout: [float, 0.1, DR]
    w_init: [str, null, WI]

pooling:
  GAP:
    axis: [int, 1, A]
  AP:
    dim: [int, 16, D]
    expansion: [int, 4, E]
    n_heads: [int, 4, H]
    dropout: [float, 0.1, DR]
    qkv_bias: [bool, 1, AB]
    k: [int, 1, K]
  GP:
    gidx: [int, 0, G]

model:
  ViT:
    patch_size: [int, 4, PS]
    depth: [int, 4, L]
    dim: [int, 16, D]
    expansion: [int, 4, E]
    n_heads: [int, 4, H]
    dropout: [float, 0.1, DR]
    drop_path: [float, 0.1, DP]
    layer_scale: [float, 1.0e-4, LS]
    qkv_bias: [bool, 1, AB]
    w_init: [str, null, WI]
  GoB:
    depth: [int, 4, L]
    dim: [int, 16, D]
    expansion: [int, 4, E]
    n_heads: [int, 4, H]
    dropout: [float, 0.1, DR]
    drop_path: [float, 0.1, DP]
    layer_scale: [float, 1.0e-4, LS]
    qkv_bias: [bool, T1, AB]
    w_init: [str, null, WI] 
  Mixer:
    patch_size: [int, 4, PS]
    depth: [int, 4, L]
    dim: [int, 16, D]
    expansion: [int, 4, E]
    n_heads: [int, 4, H]
    dropout: [float, 0.1, DR]
    drop_path: [float, null, DP]
    layer_scale: [float, null, LS]
    w_init: [str, null, WI]
  FvT:
    depth: [int, 4, L]
    dim: [int, 16, D]
    expansion: [int, 4, E]
    n_heads: [int, 4, H]
    dropout: [float, 0.1, DR]
    drop_path: [float, 0.1, DP]
    layer_scale: [float, 1.0e-4, LS]
    qkv_bias: [bool, 1, AB]
    w_init: [str, null, WI]
  PN:
    depth: [int, 3, L]
    in_depth: [int, 3, I]
    dim: [int, 64, D]
    expansion: [int, 2, E]
    K: [int, 16, K]


















