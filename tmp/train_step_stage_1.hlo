HloModule train_step_pipeshard_parallel_mesh_1.82-layer_1

%fused_computation (param_0.1: f32[32,1024], param_1.2: f32[32,1024]) -> f32[32,1024] {
  %constant_6 = f32[] constant(2), metadata={op_type="mul" op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/mul" source_file="/home/morinaga/work/alpa/tests/pipeline_parallel/test_mlp.py" source_line=29}
  %broadcast.2 = f32[32,1024]{1,0} broadcast(f32[] %constant_6), dimensions={}
  %param_0.1 = f32[32,1024]{1,0} parameter(0)
  %param_1.2 = f32[32,1024]{1,0} parameter(1)
  %subtract.1 = f32[32,1024]{1,0} subtract(f32[32,1024]{1,0} %param_0.1, f32[32,1024]{1,0} %param_1.2), metadata={op_type="sub" op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/sub" source_file="/home/morinaga/work/alpa/tests/pipeline_parallel/test_mlp.py" source_line=29}
  ROOT %multiply.3 = f32[32,1024]{1,0} multiply(f32[32,1024]{1,0} %broadcast.2, f32[32,1024]{1,0} %subtract.1), metadata={op_type="mul" op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/mul" source_file="/home/morinaga/work/alpa/tests/pipeline_parallel/test_mlp.py" source_line=29}
}

%fused_computation.1 (param_0.5: f32[1,32,512], param_1.3: f32[1,32,512]) -> f32[32,1024] {
  %param_0.5 = f32[1,32,512]{2,1,0} parameter(0)
  %param_1.3 = f32[1,32,512]{2,1,0} parameter(1)
  %concatenate.1 = f32[2,32,512]{2,1,0} concatenate(f32[1,32,512]{2,1,0} %param_0.5, f32[1,32,512]{2,1,0} %param_1.3), dimensions={0}
  %transpose.1 = f32[32,2,512]{2,0,1} transpose(f32[2,32,512]{2,1,0} %concatenate.1), dimensions={1,0,2}
  %copy.1 = f32[32,2,512]{2,1,0} copy(f32[32,2,512]{2,0,1} %transpose.1)
  ROOT %bitcast.3 = f32[32,1024]{1,0} bitcast(f32[32,2,512]{2,1,0} %copy.1)
}

%fused_computation.2 (param_0.7: f32[64,512]) -> (f32[1,32,512], f32[1,32,512]) {
  %param_0.7 = f32[64,512]{1,0} parameter(0)
  %bitcast.4 = f32[2,32,512]{2,1,0} bitcast(f32[64,512]{1,0} %param_0.7)
  %slice.3 = f32[1,32,512]{2,1,0} slice(f32[2,32,512]{2,1,0} %bitcast.4), slice={[1:2], [0:32], [0:512]}
  %slice.4.clone.1 = f32[32,512]{1,0} slice(f32[64,512]{1,0} %param_0.7), slice={[0:32], [0:512]}
  %bitcast.5.clone.1 = f32[1,32,512]{2,1,0} bitcast(f32[32,512]{1,0} %slice.4.clone.1)
  ROOT %tuple.1 = (f32[1,32,512]{2,1,0}, f32[1,32,512]{2,1,0}) tuple(f32[1,32,512]{2,1,0} %slice.3, f32[1,32,512]{2,1,0} %bitcast.5.clone.1)
}

%fused_computation.4 (param_0.10: f32[1024,1024], param_1.5: u32[]) -> f32[1024,512] {
  %param_0.10 = f32[1024,1024]{1,0} parameter(0)
  %constant_8 = s32[] constant(0)
  %param_1.5 = u32[] parameter(1)
  %convert.1 = s32[] convert(u32[] %param_1.5)
  %constant_7 = s32[] constant(512)
  %multiply.4 = s32[] multiply(s32[] %convert.1, s32[] %constant_7)
  ROOT %dynamic-slice.2 = f32[1024,512]{1,0} dynamic-slice(f32[1024,1024]{1,0} %param_0.10, s32[] %constant_8, s32[] %multiply.4), dynamic_slice_sizes={1024,512}
}

ENTRY %train_step_pipeshard_parallel_mesh_1.82-layer_1_spmd (param: f32[64,1024], param.1: f32[1024,1024], param.2: f32[512], param.3: f32[1024,1024], param.4: f32[1024], param.5: f32[32,1024]) -> (f32[32,1024], f32[64,512], s32[]) {
  %param = f32[64,1024]{1,0} parameter(0), sharding={replicated}, metadata={op_type="start" op_name="layer_1"}
  %param.1 = f32[1024,1024]{1,0} parameter(1), sharding={replicated}, metadata={op_type="start" op_name="layer_1"}
  %partition-id = u32[] partition-id()
  %fusion.4 = f32[1024,512]{1,0} fusion(f32[1024,1024]{1,0} %param.1, u32[] %partition-id), kind=kLoop, calls=%fused_computation.4
  %param.2 = f32[512]{0} parameter(2), sharding={devices=[2]0,1}, metadata={op_type="start" op_name="layer_1"}
  %broadcast.1 = f32[64,512]{1,0} broadcast(f32[512]{0} %param.2), dimensions={1}, metadata={op_type="add" op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/add" source_file="/data/morinaga/cache/pypoetry/virtualenvs/gob-gyPZHm3D-py3.8/lib/python3.8/site-packages/flax/linen/linear.py" source_line=195}
  %cublas-gemm.3 = f32[64,512]{1,0} custom-call(f32[64,1024]{1,0} %param, f32[1024,512]{1,0} %fusion.4, f32[64,512]{1,0} %broadcast.1), custom_call_target="__cublas$gemm", metadata={op_type="add" op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/add" source_file="/data/morinaga/cache/pypoetry/virtualenvs/gob-gyPZHm3D-py3.8/lib/python3.8/site-packages/flax/linen/linear.py" source_line=195}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"65536\",\"rhs_stride\":\"524288\",\"selected_algorithm\":\"114\"}"
  %fusion.2 = (f32[1,32,512]{2,1,0}, f32[1,32,512]{2,1,0}) fusion(f32[64,512]{1,0} %cublas-gemm.3), kind=kLoop, calls=%fused_computation.2
  %get-tuple-element.3 = f32[1,32,512]{2,1,0} get-tuple-element((f32[1,32,512]{2,1,0}, f32[1,32,512]{2,1,0}) %fusion.2), index=1
  %get-tuple-element.2 = f32[1,32,512]{2,1,0} get-tuple-element((f32[1,32,512]{2,1,0}, f32[1,32,512]{2,1,0}) %fusion.2), index=0
  %all-to-all.1 = (f32[1,32,512]{2,1,0}, f32[1,32,512]{2,1,0}) all-to-all(f32[1,32,512]{2,1,0} %get-tuple-element.3, f32[1,32,512]{2,1,0} %get-tuple-element.2), channel_id=1, replica_groups={{0,1}}
  %get-tuple-element = f32[1,32,512]{2,1,0} get-tuple-element((f32[1,32,512]{2,1,0}, f32[1,32,512]{2,1,0}) %all-to-all.1), index=0
  %get-tuple-element.1 = f32[1,32,512]{2,1,0} get-tuple-element((f32[1,32,512]{2,1,0}, f32[1,32,512]{2,1,0}) %all-to-all.1), index=1
  %fusion.1 = f32[32,1024]{1,0} fusion(f32[1,32,512]{2,1,0} %get-tuple-element, f32[1,32,512]{2,1,0} %get-tuple-element.1), kind=kLoop, calls=%fused_computation.1
  %param.3 = f32[1024,1024]{1,0} parameter(3), sharding={replicated}, metadata={op_type="start" op_name="layer_1"}
  %param.4 = f32[1024]{0} parameter(4), sharding={replicated}, metadata={op_type="start" op_name="layer_1"}
  %broadcast.3 = f32[32,1024]{1,0} broadcast(f32[1024]{0} %param.4), dimensions={1}
  %cublas-gemm.7 = f32[32,1024]{1,0} custom-call(f32[32,1024]{1,0} %fusion.1, f32[1024,1024]{1,0} %param.3, f32[32,1024]{1,0} %broadcast.3), custom_call_target="__cublas$gemm", metadata={op_type="add" op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/add" source_file="/data/morinaga/cache/pypoetry/virtualenvs/gob-gyPZHm3D-py3.8/lib/python3.8/site-packages/flax/linen/linear.py" source_line=195}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"lhs_stride\":\"32768\",\"rhs_stride\":\"1048576\",\"selected_algorithm\":\"102\"}"
  %param.5 = f32[32,1024]{1,0} parameter(5), sharding={devices=[2,1]0,1}, metadata={op_type="start" op_name="layer_1"}
  %fusion = f32[32,1024]{1,0} fusion(f32[32,1024]{1,0} %cublas-gemm.7, f32[32,1024]{1,0} %param.5), kind=kLoop, calls=%fused_computation, metadata={op_type="mul" op_name="parallelize(train_step_pipeshard_parallel_mesh_1)/mul" source_file="/home/morinaga/work/alpa/tests/pipeline_parallel/test_mlp.py" source_line=29}
  %constant_1 = s32[] constant(0)
  %copy.2 = s32[] copy(s32[] %constant_1)
  ROOT %tuple.2 = (f32[32,1024]{1,0}, f32[64,512]{1,0}, s32[]) tuple(f32[32,1024]{1,0} %fusion, f32[64,512]{1,0} %cublas-gemm.3, s32[] %copy.2)
}

